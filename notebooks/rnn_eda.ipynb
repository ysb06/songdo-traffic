{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers and Missing Data (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 실험 환경 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from metr.components import Metadata, TrafficData\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 음수 부호 깨짐 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 데이터 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_DATA_OF_SENSORS = \"../datasets/metr-imc/nodelink/imc_link.shp\"\n",
    "TRAFFIC_RAW_PATH = \"../datasets/metr-imc/metr-imc.h5\"\n",
    "METADATA_RAW_PATH = \"../datasets/metr-imc/metadata.h5\"\n",
    "OUTLIER_OUTPUT_DIR = \"./output/outlier_processed\"\n",
    "INTERPOLATED_OUTPUT_DIR = \"./output/interpolated\"\n",
    "FINAL_OUTPUT_DIR = \"./output/final\"\n",
    "PREDICTION_OUTPUT_DIR = \"./output/prediction\"\n",
    "\n",
    "os.makedirs(OUTLIER_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(INTERPOLATED_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PREDICTION_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw 데이터 로딩\n",
    "\n",
    "Raw 데이터는 원래 인천시에서 제공되었던 형태에서 현재 METR-LA와 같은 형태로 변환되어 있다. 행은 시간을 나타내고 열은 각 도로의 센서를 나타낸다. 형식은 전국표준노드링크의 Link ID를 나타내며 string 형식으로 지정되어 있다.\n",
    "\n",
    "이 연구에서 사용되는 모든 데이터는 아래와 같은 형태를 사용한다. 이것은 METR-LA 데이터셋과 유사한 구조이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = TrafficData.import_from_hdf(TRAFFIC_RAW_PATH)\n",
    "raw_data = raw.data\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 표준노드링크는 전국의 데이터를 포함하며 아래는 그 중 인천시에 해당하는 데이터만 추출한 데이터이다. 해당 데이터는 2024년 3월 25일에 최신화된 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_gdf = gpd.read_file(MAP_DATA_OF_SENSORS)\n",
    "print(geo_gdf.shape)\n",
    "geo_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 인천시에서는 모든 도로에 대해 교통량을 측정하지 않으며 교통량을 측정하고 있는 도로는 다음과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_gdf에서 LINK_ID가 raw_df.columns에 있는 것만 남기기\n",
    "geo_gdf_with_traffic: gpd.GeoDataFrame = geo_gdf[geo_gdf[\"LINK_ID\"].isin(raw_data.columns)]\n",
    "\n",
    "print(geo_gdf_with_traffic.shape)\n",
    "geo_gdf_with_traffic.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메타데이터는 표준노드링크에 명시된 내용을 추출했으며 각 센서에 해당하는 도로의 정보를 포함한다. 본 연구에서는 도로 허용 용량을 계산할 때에만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_metadata_df = Metadata.import_from_hdf(METADATA_RAW_PATH).data\n",
    "raw_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본적인 결측치 처리\n",
    "\n",
    "결측치 비율이 전체 데이터의 50% 이상인 데이터를 선정하여 보간이 제대로 이루어 질 수 있을 정도의 데이터를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 총 개수\n",
    "total_raw_length = raw_data.shape[0]\n",
    "total_raw_length\n",
    "\n",
    "# 50% 이상의 결측값을 가진 센서 제거\n",
    "filtered_raw_df = raw_data.dropna(thresh=int(total_raw_length * 0.5), axis=1)\n",
    "filtered_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거된 센서 개수\n",
    "print(\"Removed:\", raw_data.shape[1] - filtered_raw_df.shape[1])\n",
    "print(\"Ratio:\", (raw_data.shape[1] - filtered_raw_df.shape[1]) / raw_data.shape[1] * 100, \"%\")\n",
    "removed_sensors = raw_data.columns.difference(filtered_raw_df.columns)\n",
    "removed_sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset:\", filtered_raw_df.shape)\n",
    "print(\"Description of the Incheon dataset\")\n",
    "print(filtered_raw_df.reset_index(drop=True).melt()[\"value\"].describe().apply(lambda x: format(x, '.4f')))\n",
    "print()\n",
    "\n",
    "total_missing_values_count = filtered_raw_df.isnull().sum().sum()\n",
    "print(\"Total missing values count:\", total_missing_values_count, f\"({total_missing_values_count / filtered_raw_df.size * 100:.4f}%)\")\n",
    "print()\n",
    "\n",
    "sensors_with_mv = filtered_raw_df.isnull().any(axis=0)\n",
    "print(\"Sensors with missing values count:\", sensors_with_mv.sum(), f\"({sensors_with_mv.sum() / filtered_raw_df.shape[1] * 100:.4f}%)\")\n",
    "print(\"Sensors with no missing values count:\", (~sensors_with_mv).sum(), f\"({(~sensors_with_mv).sum() / filtered_raw_df.shape[1] * 100:.4f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_by_time(df: pd.DataFrame, title: str, size: Tuple=(10, 6)):\n",
    "    groups = df.groupby([df.index.hour, df.index.minute, df.index.second])\n",
    "    mean_df = groups.apply(lambda x: x.stack().mean())\n",
    "    mean_df.index = [\n",
    "        datetime(year=1970, month=1, day=1, hour=h, minute=m, second=s)\n",
    "        for h, m, s in mean_df.index\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=size)\n",
    "    sns.lineplot(x=mean_df.index, y=mean_df.values)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    # x축을 00:00:00부터 23:00:00까지 설정\n",
    "    ax.set_xlim([datetime(1970, 1, 1, 0, 0, 0), datetime(1970, 1, 1, 23, 0, 0)])\n",
    "    # 1시간 간격으로 눈금 표시\n",
    "    ax.xaxis.set_major_locator(mdates.HourLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H\"))\n",
    "    \n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Average Value\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_by_time(df: pd.DataFrame, title: str, size: Tuple=(12, 6)):\n",
    "    plt.figure(figsize=size)\n",
    "    sns.lineplot(x=df.index, y=df.values)\n",
    "    # plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n",
    "\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Average Value\")\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_by_days(df: pd.DataFrame, name: str):\n",
    "    data_list = []\n",
    "\n",
    "    for day in range(7):\n",
    "        day_data = df[df.index.dayofweek == day]\n",
    "        numeric_cols = day_data.select_dtypes(include=[np.number]).columns\n",
    "        day_numeric = day_data[numeric_cols]\n",
    "        values = day_numeric.values.flatten()\n",
    "        values = values[~np.isnan(values)]\n",
    "        df_day = pd.DataFrame({\"Volumes\": values, \"Days\": day})\n",
    "        data_list.append(df_day)\n",
    "\n",
    "    df_all = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "    # 요일 레이블 매핑\n",
    "    day_labels = {0: \"Mon\", 1: \"Tue\", 2: \"Wed\", 3: \"Thu\", 4: \"Fri\", 5: \"Sat\", 6: \"Sun\"}\n",
    "    df_all[\"Days\"] = df_all[\"Days\"].map(day_labels)\n",
    "    df_all[\"Days\"] = pd.Categorical(\n",
    "        df_all[\"Days\"],\n",
    "        categories=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(\n",
    "        x=\"Days\",\n",
    "        y=\"Volumes\",\n",
    "        data=df_all,\n",
    "        order=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\n",
    "    )\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Traffic Volumes\")\n",
    "    plt.title(f\"Data by Day in {name}\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    formatter = FuncFormatter(lambda x, pos: f\"{int(x):,}\")\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_hist(\n",
    "    df: pd.DataFrame,\n",
    "    name: str,\n",
    "    exclude_zero: bool = False,\n",
    "    y_max: float = None,\n",
    "    x_max: float = None,\n",
    "    bins: int = 50,\n",
    "):\n",
    "    values = df.values.flatten()\n",
    "    values = values[~np.isnan(values)]\n",
    "\n",
    "    if exclude_zero:\n",
    "        values = values[values != 0]\n",
    "\n",
    "    if x_max is not None:\n",
    "        bin_edges = np.histogram_bin_edges(\n",
    "            values, bins=bins, range=(values.min(), x_max)\n",
    "        )\n",
    "        bin_edges = np.append(bin_edges, np.inf)\n",
    "    else:\n",
    "        bin_edges = bins\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(values, bins=bin_edges, kde=True)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.title(f\"Histogram of {name}\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    if y_max is not None:\n",
    "        plt.ylim(top=y_max)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    formatter = FuncFormatter(lambda x, _: f\"{int(x):,}\")\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그래프는 전체 데이터를 평균을 내고 추세를 본 것이다. 일반적으로 대부분의 각 센서의 데이터는 아래와 같은 형태를 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_by_time(filtered_raw_df, \"Plots by Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 요일 별 데이터의 분포를 나타낸 것이다. 2024년 3월 이전의 데이터는 말도 안 되는 값(>100000)들이 많이 있는 것을 확인하기도 했지만 현재 범위(2024-03 ~ 2024-09)의 데이터는 극단적인 값들은 크게 없는 것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_days(filtered_raw_df, \"Plots by Days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 데이터 구간 별 데이터 분포를 나타낸다. 20000만개 이상은 그래프에 표시하지 않았지만 대부분의 데이터는 교통량이 2000아래에 분포해 있다. 또한 교통량이 2000이상 넘어가는 도로는 대체로 고속도로이다. 다만, 이상치도 포함하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(filtered_raw_df, \"Data Histogram\", y_max=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 데이터의 그래프는 아래와 같다. 특정 기간에서 데이터가 증가한 것을 확인할 수 있다. 그 외에도 어떤 그래프의 경우 말도 안되는 값을 가지는 경우도 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = filtered_raw_df.iloc[:, 1]\n",
    "plot_by_time(target_data, f\"Plots by Time: {target_data.name}\")\n",
    "geo_gdf[geo_gdf[\"LINK_ID\"] == target_data.name].explore(\n",
    "    style_kwds={\n",
    "        \"color\": \"red\",  # 선 색상\n",
    "        \"weight\": 5,  # 선 굵기 (픽셀 단위)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고속도로\n",
    "target_data = filtered_raw_df.loc[:, \"1610002900\"]\n",
    "plot_by_time(target_data, f\"Plots by Time: {target_data.name}\")\n",
    "geo_gdf[geo_gdf[\"LINK_ID\"] == target_data.name].explore(\n",
    "    style_kwds={\n",
    "        \"color\": \"red\",  # 선 색상\n",
    "        \"weight\": 5,  # 선 굵기 (픽셀 단위)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2024년 3월 이전의 데이터는 100,000이 넘어가는 비정상적 데이터도 확인했지만 현재 범위의 데이터에서는 비정상적으로는 보이지 않는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while idx < filtered_raw_df.shape[1]:    \n",
    "    idx += 1\n",
    "    target_data = filtered_raw_df.iloc[:, idx]\n",
    "    if target_data[target_data > 10000].any():\n",
    "        break\n",
    "\n",
    "plot_by_time(target_data, f\"Plots by Time: {target_data.name}\")\n",
    "geo_gdf[geo_gdf[\"LINK_ID\"] == target_data.name].explore(\n",
    "    style_kwds={\n",
    "        \"color\": \"red\",  # 선 색상\n",
    "        \"weight\": 5,  # 선 굵기 (픽셀 단위)\n",
    "    },\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"1640332801\"과 같은 센서의 경우 결측치가 대부분이고 유효한 값은 거의 없는 것으로 보인다. 해당 위치의 경우 차량통행이 꽤 있는 곳이며 측정이 제대로 이루어지지 않았음을 예측할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while idx < filtered_raw_df.shape[1]:    \n",
    "    idx += 1\n",
    "    target_data = filtered_raw_df.iloc[:, idx]\n",
    "    if target_data.isna().sum() > 1000:\n",
    "        break\n",
    "\n",
    "plot_by_time(target_data, f\"Plots by Time: {target_data.name}\")\n",
    "geo_gdf[geo_gdf[\"LINK_ID\"] == target_data.name].explore(\n",
    "    style_kwds={\n",
    "        \"color\": \"red\",  # 선 색상\n",
    "        \"weight\": 5,  # 선 굵기 (픽셀 단위)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outliers 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 Outlier 처리\n",
    "\n",
    "앞서 본 연구에서 다음 이상치 처리를 기본으로 적용한다.\n",
    "\n",
    "1. 결측치 주변 0을 제거\n",
    "  경험적 데이터 분석 결과를 바탕으로 결측치 주변 0이 \n",
    "2. 이론적 도로 허용 용량 기반 제거 (1.5배 또는 2.0배까지 허용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from songdo_rnn.preprocessing.outlier import remove_base_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = remove_base_outliers(\n",
    "    filtered_raw_df,\n",
    "    start_datetime=None,\n",
    "    end_datetime=None,\n",
    "    traffic_capacity_adjustment_rate=2.0,\n",
    "    metadata_path=METADATA_RAW_PATH,\n",
    ")\n",
    "result_path = os.path.join(OUTLIER_OUTPUT_DIR, \"base.h5\")\n",
    "result.to_hdf(result_path, key=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = TrafficData.import_from_hdf(result_path).data\n",
    "base_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 가능한 데이터만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = \"2024-08-01\"\n",
    "test_end = \"2024-08-31\"\n",
    "test_candidates = filtered_raw_df.loc[test_start:test_end, :]\n",
    "test_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_exist_list = test_candidates.columns[test_candidates.isna().any()]\n",
    "intersection_list = list(base_df.columns.intersection(na_exist_list))\n",
    "print(len(na_exist_list))\n",
    "print(\"NaNs:\", len(intersection_list), \"/\", len(base_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_intersection_columns = base_df.columns.difference(intersection_list)\n",
    "training_df = base_df[non_intersection_columns]\n",
    "training_df = training_df.loc[:pd.Timestamp(test_start), :]\n",
    "training_df = training_df.iloc[:-1, :]\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = filtered_raw_df.loc[test_start:test_end, training_df.columns]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 Outlier 처리 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fromto_df(df1: pd.Series, df2: pd.Series, title: str = \"\"):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df1.plot(label=\"Original Data\", color=\"red\")\n",
    "    df2.plot(label=\"Processed Data\", color=\"blue\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1\n",
    "\n",
    "target_data = training_df.iloc[:, idx]\n",
    "original_data = raw_data[target_data.name]\n",
    "\n",
    "\n",
    "compare_fromto_df(original_data, target_data, title=f\"Origin vs Target: {original_data.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while idx < training_df.shape[1]:\n",
    "    idx += 1\n",
    "    target_data = training_df.iloc[:, idx]\n",
    "    original_data = filtered_raw_df[target_data.name]\n",
    "    original_data = original_data.loc[target_data.index.min():target_data.index.max()]\n",
    "    \n",
    "\n",
    "    if original_data.isna().sum() == target_data.isna().sum():\n",
    "        continue\n",
    "\n",
    "    print(original_data.name, \":\" , original_data.isna().sum())\n",
    "    print(target_data.name, \":\", target_data.isna().sum())\n",
    "\n",
    "    compare_fromto_df(original_data, target_data, title=f\"Origin vs Target: {original_data.name}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "end_idx = 0\n",
    "\n",
    "for idx in range(target_data.shape[0]):\n",
    "    if pd.isna(target_data.iloc[idx]):\n",
    "        start_idx = idx - 10\n",
    "        break\n",
    "\n",
    "\n",
    "for idx in range(target_data.shape[0] - 1, -1, -1):\n",
    "    if pd.isna(target_data.iloc[idx]):\n",
    "        end_idx = idx + 10\n",
    "        break\n",
    "\n",
    "start_idx = start_idx if start_idx > 0 else 0\n",
    "end_idx = end_idx if end_idx < target_data.shape[0] else target_data.shape[0]\n",
    "\n",
    "original_cut_data = original_data.iloc[start_idx:end_idx]\n",
    "target_cut_data = target_data.iloc[start_idx:end_idx]\n",
    "\n",
    "compare_fromto_df(original_cut_data, target_cut_data, title=f\"Origin vs Target: {original_data.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 0제거 외에 허용 용량에 따른 제거된 사례를 확인하는 코드임. 테스트 상 2.0배를 넘어가는 경우는 종종 있지만 2.5를 넘어가지는 않음. 대체로 속도가 바뀌는 지점의 도로에서 과속이 많이 발생하고 이에 따라 허용 용량 이상으로 차량이 통과하는 것으로 보임. 따라서, 실제 통과 차량의 한계를 얼마나 정할 것인지 정할 필요가 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while idx < training_df.shape[1] - 1:\n",
    "    idx += 1\n",
    "    target_data = training_df.iloc[:, idx]\n",
    "    original_data = filtered_raw_df[target_data.name]\n",
    "    original_data = original_data.loc[target_data.index.min():target_data.index.max()]\n",
    "    \n",
    "\n",
    "    if original_data.isna().sum() == target_data.isna().sum():\n",
    "        continue\n",
    "    \n",
    "    # 0이 아닌 값 중 NaN이 아닌 값만 추출\n",
    "    # 즉 실제 삭제된 결측치만 추출\n",
    "    not_0_and_na = original_data[original_data != 0]\n",
    "    not_0_and_na = not_0_and_na[not_0_and_na.notna()]\n",
    "    na_target = target_data[not_0_and_na.index]\n",
    "    if na_target.isna().sum() == 0:\n",
    "        continue\n",
    "\n",
    "    print(original_data.name, \"(Origin):\" , original_data.isna().sum())\n",
    "    print(target_data.name, \"(Target):\", target_data.isna().sum())\n",
    "    metadata = raw_metadata_df[raw_metadata_df[\"LINK_ID\"] == original_data.name]\n",
    "    print(metadata)\n",
    "\n",
    "    # Capacity 계산 과정\n",
    "    speed_limit = metadata[\"MAX_SPD\"].values[0]\n",
    "    alpha = 10 * (100 - speed_limit)\n",
    "    if speed_limit > 100:\n",
    "        alpha /= 2\n",
    "    lane_count = metadata[\"LANES\"].values[0]\n",
    "    adjustment_rate = 2.0\n",
    "    original_capacity = (2200 - alpha) * lane_count\n",
    "    capacity = original_capacity * adjustment_rate\n",
    "    print(\"Capacity:\", capacity, f\"({original_capacity} x {adjustment_rate})\")\n",
    "    exceed_capacity_data = target_data[target_data > capacity]\n",
    "    print(exceed_capacity_data if exceed_capacity_data.size > 0 else \"No exceed capacity data\")\n",
    "\n",
    "    compare_fromto_df(original_data, target_data, title=f\"Origin vs Target: {original_data.name}\")\n",
    "    \n",
    "    break\n",
    "\n",
    "print(original_data.name)\n",
    "geo_gdf[geo_gdf[\"LINK_ID\"] == original_data.name].explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터 임의 결측치 생성\n",
    "\n",
    "rnn_outliers에서 좀 더 개선되고 변경된 코드로 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에 NaN이 없는지 다시 확인\n",
    "test_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 데이터의 범위\n",
    "t_all = training_df.values.flatten()\n",
    "t_all = t_all[~np.isnan(t_all)]\n",
    "t_mean = t_all.mean()\n",
    "t_std = t_all.std()\n",
    "t_z = (t_all - t_mean) / t_std\n",
    "t_outlier_indices = np.where(np.abs(t_z) > 3)[0]\n",
    "t_outliers = t_all[t_outlier_indices]\n",
    "\n",
    "# 이상치 데이터의 범위와 통계 출력\n",
    "if len(t_outliers) > 0:\n",
    "    min_outlier = t_outliers.min()\n",
    "    max_outlier = t_outliers.max()\n",
    "    print(f\"이상치 데이터의 개수: {len(t_outliers)}\")\n",
    "    print(f\"이상치 데이터의 범위: {min_outlier:.2f}에서 {max_outlier:.2f}\")\n",
    "    print(f\"전체 데이터의 평균: {t_mean:.2f}, 표준편차: {t_std:.2f}\")\n",
    "    print(\n",
    "        f\"전체 데이터의 정상 범위: {t_mean - 3 * t_std:.2f}에서 {t_mean + 3 * t_std:.2f}\"\n",
    "    )\n",
    "    print(f\"이상치 비율: {len(t_outliers) / len(t_all) * 100:.2f}%\")\n",
    "else:\n",
    "    print(\"z-score가 3 이상인 이상치 데이터가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 결측치 비율\n",
    "missing_ratio = training_df.isna().sum().sum() / training_df.size\n",
    "print(f\"전체 결측치 비율: {missing_ratio:.4f} ({missing_ratio*100:.2f}%)\")\n",
    "\n",
    "# 열별 결측치 비율\n",
    "column_missing = training_df.isna().mean()\n",
    "print(\"\\n열별 결측치 비율:\")\n",
    "print(column_missing)\n",
    "\n",
    "# 결측치가 가장 많은 열 확인\n",
    "print(\"\\n결측치가 가장 많은 열 Top 5:\")\n",
    "print(column_missing.sort_values(ascending=False).head())\n",
    "\n",
    "# 행별 결측치 비율 분포 확인\n",
    "row_missing = training_df.isna().mean(axis=1)\n",
    "print(f\"\\n행별 결측치 비율 평균: {row_missing.mean():.4f}\")\n",
    "print(f\"행별 결측치 비율 최댓값: {row_missing.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_outliers.sort()\n",
    "t_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df의 copy 생성\n",
    "# 해당 copy에 이상치를 위에서 구한 이상치 비율만큼 데이터를 랜덤 선택하고 t_outliers의 랜덤을 선택한 어느 한 값으로 대체\n",
    "# 위 데이터에 위에서 구한 전체 결측치 비율만큼 데이터를 랜덤 선택하고 NaN 결측치로 대체\n",
    "\n",
    "# test_df의 copy 생성\n",
    "corrupted_test_df = test_df.copy()\n",
    "\n",
    "# 1. 이상치 비율 계산 (이전 셀에서 이미 계산됨)\n",
    "outlier_ratio = len(t_outliers) / len(t_all)\n",
    "\n",
    "# 2. 결측치 비율 계산\n",
    "missing_ratio = training_df.isna().sum().sum() / training_df.size\n",
    "\n",
    "# 3. 이상치 추가 - 이상치 비율만큼 랜덤 데이터 선택하고 t_outliers 값으로 대체\n",
    "# 전체 데이터 포인트 개수\n",
    "total_points = corrupted_test_df.size\n",
    "# 추가할 이상치 개수\n",
    "n_outliers = int(total_points * outlier_ratio)\n",
    "\n",
    "# 랜덤으로 위치 선택 (행, 열 인덱스)\n",
    "random_indices = np.random.choice(total_points, n_outliers, replace=False)\n",
    "rows = random_indices // corrupted_test_df.shape[1]\n",
    "cols = random_indices % corrupted_test_df.shape[1]\n",
    "\n",
    "# 랜덤 이상치 값 선택하여 대체\n",
    "for i in range(n_outliers):\n",
    "    # t_outliers에서 랜덤하게 하나 선택\n",
    "    outlier_value = np.random.choice(t_outliers)\n",
    "    r, c = rows[i], cols[i]\n",
    "    corrupted_test_df.iloc[r, c] = outlier_value\n",
    "\n",
    "# 4. 결측치 추가 - 결측치 비율만큼 랜덤 데이터 선택하고 NaN으로 대체\n",
    "# 추가할 결측치 개수\n",
    "n_missing = int(total_points * missing_ratio)\n",
    "\n",
    "# 이상치와 겹치지 않도록 나머지 데이터 중에서 선택\n",
    "remaining_indices = np.setdiff1d(np.arange(total_points), random_indices)\n",
    "missing_indices = np.random.choice(remaining_indices, n_missing, replace=False)\n",
    "\n",
    "# 결측치 위치 계산\n",
    "missing_rows = missing_indices // corrupted_test_df.shape[1]\n",
    "missing_cols = missing_indices % corrupted_test_df.shape[1]\n",
    "\n",
    "# NaN으로 대체\n",
    "for i in range(n_missing):\n",
    "    r, c = missing_rows[i], missing_cols[i]\n",
    "    corrupted_test_df.iloc[r, c] = np.nan\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"원본 test_df 크기: {test_df.shape}\")\n",
    "print(f\"손상된 test_df 크기: {corrupted_test_df.shape}\")\n",
    "print(f\"추가된 이상치 개수: {n_outliers} (비율: {outlier_ratio:.4f})\")\n",
    "print(f\"추가된 결측치 개수: {n_missing} (비율: {missing_ratio:.4f})\")\n",
    "print(f\"손상된 데이터의 실제 결측치 비율: {corrupted_test_df.isna().sum().sum() / corrupted_test_df.size:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 원본과 손상된 데이터 비교 (몇 개 컬럼 샘플링)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('원본 데이터와 손상된 데이터 비교', fontsize=16)\n",
    "\n",
    "# 랜덤으로 4개 컬럼 선택\n",
    "sample_cols = np.random.choice(test_df.columns, 4, replace=False)\n",
    "\n",
    "for i, col in enumerate(sample_cols):\n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    # 원본 데이터\n",
    "    ax.plot(test_df.index, test_df[col], 'b-', alpha=0.7, label='원본')\n",
    "    \n",
    "    # 손상된 데이터\n",
    "    ax.plot(corrupted_test_df.index, corrupted_test_df[col], 'r.', alpha=0.7, label='손상됨')\n",
    "    \n",
    "    ax.set_title(f'컬럼: {col}')\n",
    "    ax.legend()\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# 2. 결측치 히트맵\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('결측치 분포 히트맵', fontsize=16)\n",
    "sns.heatmap(corrupted_test_df.isna(), cmap='viridis', cbar_kws={'label': '결측 여부'})\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. 이상치 시각화\n",
    "# z-score 계산\n",
    "def get_zscore(df):\n",
    "    return (df - df.mean()) / df.std()\n",
    "\n",
    "z_scores = get_zscore(corrupted_test_df)\n",
    "is_outlier = (np.abs(z_scores) > 3) & (~corrupted_test_df.isna())\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('이상치(|z-score| > 3) 분포 히트맵', fontsize=16)\n",
    "sns.heatmap(is_outlier, cmap='OrRd', cbar_kws={'label': '이상치 여부'})\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. 데이터 분포 비교 (상자 그림) - 수정된 버전\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.title('원본 vs 손상된 데이터 분포 비교 (상자 그림)', fontsize=16)\n",
    "\n",
    "# 인덱스 리셋 및 일부 특성만 선택하여 시각화\n",
    "# sample_cols에서 이미 선택한 4개 컬럼만 사용\n",
    "orig_sample = test_df[sample_cols].reset_index()\n",
    "corr_sample = corrupted_test_df[sample_cols].reset_index()\n",
    "\n",
    "# 데이터 재구성\n",
    "orig_melt = orig_sample.melt(id_vars='index', var_name='특성', value_name='값')\n",
    "orig_melt['데이터셋'] = '원본'\n",
    "corr_melt = corr_sample.melt(id_vars='index', var_name='특성', value_name='값')\n",
    "corr_melt['데이터셋'] = '손상됨'\n",
    "\n",
    "# 결합 및 시각화\n",
    "combined = pd.concat([orig_melt, corr_melt]).reset_index(drop=True)\n",
    "sns.boxplot(x='특성', y='값', hue='데이터셋', data=combined, showfliers=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. 일부 특성에 대한 값 분포 비교 (히스토그램)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('원본과 손상된 데이터의 분포 비교 (히스토그램)', fontsize=16)\n",
    "\n",
    "for i, col in enumerate(sample_cols):\n",
    "    ax = axes[i//2, i%2]\n",
    "    sns.histplot(test_df[col], color='blue', alpha=0.5, label='원본', ax=ax)\n",
    "    sns.histplot(corrupted_test_df[col], color='red', alpha=0.5, label='손상됨', ax=ax)\n",
    "    ax.set_title(f'컬럼: {col}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = pd.concat([training_df, corrupted_test_df], axis=0, copy=True)\n",
    "print(comb_df.shape)\n",
    "print(comb_df.index.is_monotonic_increasing)\n",
    "comb_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
