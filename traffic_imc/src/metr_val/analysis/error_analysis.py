import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from typing import Dict, Tuple, List
from dataclasses import dataclass
import logging


@dataclass
class ErrorMetrics:
    """ì—ëŸ¬ ë©”íŠ¸ë¦­ì„ ì €ì¥í•˜ëŠ” ë°ì´í„°í´ë˜ìŠ¤"""
    mae: float
    rmse: float
    mape: float
    r2: float
    sensor_name: str
    data_points: int


@dataclass
class ErrorCase:
    """ê°œë³„ ì—ëŸ¬ ì¼€ì´ìŠ¤ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„°í´ë˜ìŠ¤"""
    sensor_name: str
    timestamp: pd.Timestamp
    target_value: float
    predicted_value: float
    absolute_error: float
    relative_error: float


def calculate_metrics(target: np.ndarray, prediction: np.ndarray) -> Dict[str, float]:
    """ê¸°ë³¸ íšŒê·€ ë©”íŠ¸ë¦­ë“¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    # NaN ê°’ ì œê±°
    mask = ~(np.isnan(target) | np.isnan(prediction))
    target_clean = target[mask]
    pred_clean = prediction[mask]
    
    if len(target_clean) == 0:
        return {"mae": np.nan, "rmse": np.nan, "mape": np.nan, "r2": np.nan}
    
    # MAE (Mean Absolute Error)
    mae = np.mean(np.abs(target_clean - pred_clean))
    
    # RMSE (Root Mean Square Error)
    rmse = np.sqrt(np.mean((target_clean - pred_clean) ** 2))
    
    # MAPE (Mean Absolute Percentage Error)
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‘ì€ ê°’ì„ ë”í•¨
    mape = np.mean(np.abs((target_clean - pred_clean) / (target_clean + 1e-8))) * 100
    
    # RÂ² (ê²°ì •ê³„ìˆ˜)
    ss_res = np.sum((target_clean - pred_clean) ** 2)
    ss_tot = np.sum((target_clean - np.mean(target_clean)) ** 2)
    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else np.nan
    
    return {"mae": mae, "rmse": rmse, "mape": mape, "r2": r2}


def analyze_dataset_errors(
    result: Dict[str, pd.DataFrame], 
    dataset_name: str
) -> Tuple[List[ErrorMetrics], List[ErrorCase], go.Figure]:
    """
    ë°ì´í„°ì…‹ì˜ ì˜ˆì¸¡ ì—ëŸ¬ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        result: ì„¼ì„œë³„ ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ (training, validation, test)
    
    Returns:
        sensor_metrics: ì„¼ì„œë³„ ì—ëŸ¬ ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸
        top_errors: ê°€ì¥ í° ì—ëŸ¬ Top 10 ë¦¬ìŠ¤íŠ¸
        analysis_fig: ì—ëŸ¬ ë¶„ì„ ì‹œê°í™” Figure
    """
    logger = logging.getLogger(__name__)
    logger.info(f"=== {dataset_name.upper()} ë°ì´í„°ì…‹ ì—ëŸ¬ ë¶„ì„ ì‹œì‘ ===")
    
    sensor_metrics = []
    all_errors = []
    
    # ì„¼ì„œë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
    for sensor_name, df in result.items():
        target = df['target'].values
        prediction = df['prediction'].values
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = calculate_metrics(target, prediction)
        
        sensor_metric = ErrorMetrics(
            mae=metrics['mae'],
            rmse=metrics['rmse'],
            mape=metrics['mape'],
            r2=metrics['r2'],
            sensor_name=sensor_name,
            data_points=len(df)
        )
        sensor_metrics.append(sensor_metric)
        
        # ê°œë³„ ì—ëŸ¬ ì¼€ì´ìŠ¤ ìˆ˜ì§‘
        absolute_errors = np.abs(target - prediction)
        relative_errors = np.abs((target - prediction) / (target + 1e-8)) * 100
        
        for i, (time, tar, pred, abs_err, rel_err) in enumerate(zip(
            df['time'], target, prediction, absolute_errors, relative_errors
        )):
            error_case = ErrorCase(
                sensor_name=sensor_name,
                timestamp=time,
                target_value=tar,
                predicted_value=pred,
                absolute_error=abs_err,
                relative_error=rel_err
            )
            all_errors.append(error_case)
    
    # Top 10 ì—ëŸ¬ ì¼€ì´ìŠ¤ ì¶”ì¶œ
    top_errors = sorted(all_errors, key=lambda x: x.absolute_error, reverse=True)[:10]
    
    # ì „ì²´ ë©”íŠ¸ë¦­ ìš”ì•½
    logger.info("ğŸ“Š ì „ì²´ ì„±ëŠ¥ ìš”ì•½:")
    valid_metrics = [m for m in sensor_metrics if not np.isnan(m.mae)]
    if valid_metrics:
        avg_mae = np.mean([m.mae for m in valid_metrics])
        avg_rmse = np.mean([m.rmse for m in valid_metrics])
        avg_mape = np.mean([m.mape for m in valid_metrics])
        avg_r2 = np.mean([m.r2 for m in valid_metrics])
        
        logger.info(f"  í‰ê·  MAE: {avg_mae:.4f}")
        logger.info(f"  í‰ê·  RMSE: {avg_rmse:.4f}")
        logger.info(f"  í‰ê·  MAPE: {avg_mape:.2f}%")
        logger.info(f"  í‰ê·  RÂ²: {avg_r2:.4f}")
    
    # ì„¼ì„œë³„ ì„±ëŠ¥ ì¶œë ¥
    logger.info("ğŸ“ˆ ì„¼ì„œë³„ ì„±ëŠ¥ (MAE ê¸°ì¤€ ìƒìœ„ 5ê°œ):")
    best_sensors = sorted(valid_metrics, key=lambda x: x.mae)[:5]
    for i, metric in enumerate(best_sensors, 1):
        logger.info(f"  {i}. ì„¼ì„œ {metric.sensor_name}: MAE={metric.mae:.4f}, RMSE={metric.rmse:.4f}")
    
    logger.info("ğŸ“‰ ì„¼ì„œë³„ ì„±ëŠ¥ (MAE ê¸°ì¤€ í•˜ìœ„ 5ê°œ):")
    worst_sensors = sorted(valid_metrics, key=lambda x: x.mae, reverse=True)[:5]
    for i, metric in enumerate(worst_sensors, 1):
        logger.info(f"  {i}. ì„¼ì„œ {metric.sensor_name}: MAE={metric.mae:.4f}, RMSE={metric.rmse:.4f}")
    
    # Top 10 ì—ëŸ¬ ì¼€ì´ìŠ¤ ì¶œë ¥
    logger.info("ğŸ”¥ ê°€ì¥ í° ì—ëŸ¬ Top 10:")
    for i, error in enumerate(top_errors, 1):
        logger.info(f"  {i}. ì„¼ì„œ {error.sensor_name} ({error.timestamp})")
        logger.info(f"     ì‹¤ì œê°’: {error.target_value:.2f}, ì˜ˆì¸¡ê°’: {error.predicted_value:.2f}")
        logger.info(f"     ì ˆëŒ€ ì—ëŸ¬: {error.absolute_error:.2f}, ìƒëŒ€ ì—ëŸ¬: {error.relative_error:.1f}%")
    
    # ì‹œê°í™” ìƒì„±
    analysis_fig = create_error_analysis_plot(sensor_metrics, top_errors, dataset_name)
    
    return sensor_metrics, top_errors, analysis_fig


def create_error_analysis_plot(
    sensor_metrics: List[ErrorMetrics], 
    top_errors: List[ErrorCase], 
    dataset_name: str
) -> go.Figure:
    """ì—ëŸ¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    
    # ì„œë¸Œí”Œë¡¯ ìƒì„± (2x2 ë ˆì´ì•„ì›ƒ)
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=[
            "ì„¼ì„œë³„ MAE ë¶„í¬",
            "ì„¼ì„œë³„ RMSE vs MAE",
            "Top 10 ì—ëŸ¬ ì¼€ì´ìŠ¤",
            "ì—ëŸ¬ ë©”íŠ¸ë¦­ ìƒê´€ê´€ê³„"
        ],
        specs=[[{"type": "histogram"}, {"type": "scatter"}],
               [{"type": "bar"}, {"type": "scatter"}]]
    )
    
    # ìœ íš¨í•œ ë©”íŠ¸ë¦­ë§Œ í•„í„°ë§
    valid_metrics = [m for m in sensor_metrics if not np.isnan(m.mae)]
    
    if not valid_metrics:
        fig.add_annotation(
            text="ìœ íš¨í•œ ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
            xref="paper", yref="paper",
            x=0.5, y=0.5, showarrow=False
        )
        return fig
    
    # 1. MAE ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    mae_values = [m.mae for m in valid_metrics]
    fig.add_trace(
        go.Histogram(x=mae_values, name="MAE ë¶„í¬", nbinsx=20),
        row=1, col=1
    )
    
    # 2. RMSE vs MAE ì‚°ì ë„
    rmse_values = [m.rmse for m in valid_metrics]
    sensor_names = [m.sensor_name for m in valid_metrics]
    fig.add_trace(
        go.Scatter(
            x=mae_values, y=rmse_values,
            mode='markers',
            text=sensor_names,
            name="ì„¼ì„œë³„ ì„±ëŠ¥",
            hovertemplate="<b>ì„¼ì„œ %{text}</b><br>MAE: %{x:.4f}<br>RMSE: %{y:.4f}<extra></extra>"
        ),
        row=1, col=2
    )
    
    # 3. Top 10 ì—ëŸ¬ ì¼€ì´ìŠ¤ ë°”ì°¨íŠ¸
    if top_errors:
        error_labels = [f"ì„¼ì„œ {e.sensor_name}<br>{e.timestamp.strftime('%m-%d %H:%M')}" 
                       for e in top_errors]
        error_values = [e.absolute_error for e in top_errors]
        
        fig.add_trace(
            go.Bar(
                x=list(range(len(error_values))),
                y=error_values,
                text=error_labels,
                name="Top 10 ì—ëŸ¬",
                hovertemplate="<b>%{text}</b><br>ì ˆëŒ€ ì—ëŸ¬: %{y:.2f}<extra></extra>"
            ),
            row=2, col=1
        )
    
    # 4. MAPE vs RÂ² ì‚°ì ë„
    mape_values = [m.mape for m in valid_metrics if not np.isnan(m.mape)]
    r2_values = [m.r2 for m in valid_metrics if not np.isnan(m.r2)]
    
    if len(mape_values) == len(r2_values) and len(mape_values) > 0:
        fig.add_trace(
            go.Scatter(
                x=mape_values, y=r2_values,
                mode='markers',
                text=sensor_names[:len(mape_values)],
                name="MAPE vs RÂ²",
                hovertemplate="<b>ì„¼ì„œ %{text}</b><br>MAPE: %{x:.2f}%<br>RÂ²: %{y:.4f}<extra></extra>"
            ),
            row=2, col=2
        )
    
    # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸
    fig.update_layout(
        title=f"{dataset_name.upper()} ë°ì´í„°ì…‹ ì—ëŸ¬ ë¶„ì„",
        height=800,
        showlegend=False
    )
    
    # ì¶• ë¼ë²¨ ì„¤ì •
    fig.update_xaxes(title_text="MAE", row=1, col=1)
    fig.update_yaxes(title_text="ë¹ˆë„", row=1, col=1)
    
    fig.update_xaxes(title_text="MAE", row=1, col=2)
    fig.update_yaxes(title_text="RMSE", row=1, col=2)
    
    fig.update_xaxes(title_text="ì—ëŸ¬ ìˆœìœ„", row=2, col=1)
    fig.update_yaxes(title_text="ì ˆëŒ€ ì—ëŸ¬", row=2, col=1)
    
    fig.update_xaxes(title_text="MAPE (%)", row=2, col=2)
    fig.update_yaxes(title_text="RÂ²", row=2, col=2)
    
    return fig


def save_error_analysis_results(
    sensor_metrics: List[ErrorMetrics], 
    top_errors: List[ErrorCase], 
    dataset_name: str, 
    output_dir: str
) -> None:
    """ì—ëŸ¬ ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    from pathlib import Path
    
    logger = logging.getLogger(__name__)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ì„¼ì„œë³„ ë©”íŠ¸ë¦­ ì €ì¥
    metrics_data = []
    for metric in sensor_metrics:
        metrics_data.append({
            'sensor_name': metric.sensor_name,
            'mae': metric.mae,
            'rmse': metric.rmse,
            'mape': metric.mape,
            'r2': metric.r2,
            'data_points': metric.data_points
        })
    
    metrics_df = pd.DataFrame(metrics_data)
    metrics_path = output_path / f"error_metrics_{dataset_name}.csv"
    metrics_df.to_csv(metrics_path, index=False)
    logger.info(f"ì„¼ì„œë³„ ë©”íŠ¸ë¦­ ì €ì¥: {metrics_path}")
    
    # Top ì—ëŸ¬ ì¼€ì´ìŠ¤ ì €ì¥
    errors_data = []
    for error in top_errors:
        errors_data.append({
            'sensor_name': error.sensor_name,
            'timestamp': error.timestamp,
            'target_value': error.target_value,
            'predicted_value': error.predicted_value,
            'absolute_error': error.absolute_error,
            'relative_error': error.relative_error
        })
    
    errors_df = pd.DataFrame(errors_data)
    errors_path = output_path / f"top_errors_{dataset_name}.csv"
    errors_df.to_csv(errors_path, index=False)
    logger.info(f"Top 10 ì—ëŸ¬ ì¼€ì´ìŠ¤ ì €ì¥: {errors_path}")
